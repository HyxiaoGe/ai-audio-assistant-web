# V1.2ç‰ˆæœ¬å®æ–½æŒ‡å—

## å·²å®Œæˆçš„å·¥ä½œ âœ…

### 1. è½¬å†™æ–‡æœ¬é¢„å¤„ç†å’Œè´¨é‡è¯„ä¼°å·¥å…·
- âœ… åˆ›å»º `app/utils/transcript_processor.py`
  - `TranscriptQuality` ç±»ï¼šè´¨é‡è¯„ä¼°ç»“æœ
  - `TranscriptProcessor` ç±»ï¼šæä¾›è´¨é‡è¯„ä¼°å’Œé¢„å¤„ç†åŠŸèƒ½
  - æ”¯æŒè¿‡æ»¤è¯­æ°”è¯ã€åˆå¹¶segmentã€ç”Ÿæˆè´¨é‡æç¤º

### 2. ç« èŠ‚åˆ’åˆ†æç¤ºè¯æ¨¡æ¿
- âœ… åˆ›å»º `app/prompts/templates/segmentation/` ç›®å½•
- âœ… åˆ›å»º `config.json` - ç« èŠ‚åˆ’åˆ†é…ç½®
- âœ… åˆ›å»º `zh-CN.json` - ä¸­æ–‡ç« èŠ‚åˆ’åˆ†æç¤ºè¯
  - æ”¯æŒ5ç§å†…å®¹é£æ ¼çš„ç« èŠ‚åˆ’åˆ†
  - åŒ…å«è´¨é‡å®¹é”™æŒ‡å¯¼

### 3. æ›´æ–°æ‘˜è¦æç¤ºè¯
- âœ… æ›´æ–° `app/prompts/templates/summary/config.json` åˆ°v1.2.0
  - æå‡max_tokensé™åˆ¶
  - è°ƒæ•´temperatureå‚æ•°
- âœ… æ›´æ–° `app/prompts/templates/summary/zh-CN.json`
  - å¢åŠ è´¨é‡æç¤ºå˜é‡`{quality_notice}`
  - ä¼˜åŒ–ä¸ºç»“æ„åŒ–Markdownè¾“å‡º
  - ä¸‰ç§é£æ ¼å®Œå…¨å·®å¼‚åŒ–ï¼ˆmeeting/lecture/podcastï¼‰
  - å¢åŠ è¡¨æ ¼ã€emojiç­‰ç»“æ„åŒ–å…ƒç´ 

---

## å‰©ä½™å·¥ä½œ ğŸš§

### ç¬¬ä¸€æ­¥ï¼šä¿®æ”¹ PromptManager æ”¯æŒæ–°çš„æ¨¡æ¿å˜é‡

**æ–‡ä»¶ï¼š** `app/prompts/manager.py`

**éœ€è¦ä¿®æ”¹çš„åœ°æ–¹ï¼š**

å½“å‰çš„ `get_prompt()` æ–¹æ³•å¯èƒ½ä¸æ”¯æŒ `quality_notice` è¿™ä¸ªå˜é‡ã€‚éœ€è¦ç¡®ä¿ï¼š

```python
def get_prompt(
    self,
    category: str,          # "summary" æˆ– "segmentation"
    prompt_type: str,       # "overview", "key_points", "action_items", "segment"
    locale: str = "zh-CN",
    variables: Optional[dict] = None,   # æ”¯æŒä¼ å…¥å˜é‡
    content_style: str = "meeting"
) -> dict:
    """è·å–æç¤ºè¯é…ç½®

    Returns:
        {
            "system": "ç³»ç»Ÿè§’è‰²æè¿°",
            "user_prompt": "æ ¼å¼åŒ–åçš„ç”¨æˆ·æç¤ºè¯",
            "model_params": {...}
        }
    """
```

**å®ç°è¦ç‚¹ï¼š**
1. åŠ è½½ segmentation ç±»å‹çš„æç¤ºè¯
2. æ ¹æ®content_styleé€‰æ‹©æ­£ç¡®çš„template
3. ç”¨variablesä¸­çš„å€¼æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦ï¼ˆå¦‚`{quality_notice}`, `{transcript}`ï¼‰

---

### ç¬¬äºŒæ­¥ï¼šåœ¨LLMæœåŠ¡åŸºç±»ä¸­æ·»åŠ æ–¹æ³•

**æ–‡ä»¶ï¼š** `app/services/llm/base.py`

**éœ€è¦æ·»åŠ çš„æŠ½è±¡æ–¹æ³•ï¼š**

```python
from abc import ABC, abstractmethod
from typing import AsyncIterator, Optional

class LLMService(ABC):
    # ... ç°æœ‰æ–¹æ³• ...

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """é€šç”¨æ–‡æœ¬ç”Ÿæˆ

        ç”¨äºç« èŠ‚åˆ’åˆ†ç­‰éœ€è¦è‡ªå®šä¹‰promptçš„åœºæ™¯

        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼‰

        Returns:
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        raise NotImplementedError
```

**ç„¶ååœ¨æ‰€æœ‰å…·ä½“å®ç°ä¸­æ·»åŠ æ­¤æ–¹æ³•ï¼š**
- `app/services/llm/deepseek.py`
- `app/services/llm/qwen.py`
- `app/services/llm/doubao.py`
- `app/services/llm/moonshot.py`
- `app/services/llm/openrouter.py`

**å®ç°ç¤ºä¾‹ï¼ˆä»¥DeepSeekä¸ºä¾‹ï¼‰ï¼š**

```python
# app/services/llm/deepseek.py

async def generate(
    self,
    prompt: str,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None
) -> str:
    """é€šç”¨æ–‡æœ¬ç”Ÿæˆ"""

    url = f"{self._base_url}/chat/completions"

    payload = {
        "model": self._model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature or self._temperature,
        "max_tokens": max_tokens or self._max_tokens,
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(
            url,
            json=payload,
            headers={"Authorization": f"Bearer {self._api_key}"}
        )
        response.raise_for_status()
        data = response.json()

    return data["choices"][0]["message"]["content"]
```

---

### ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹ Worker ä»»åŠ¡æµç¨‹

**æ–‡ä»¶ï¼š** `worker/tasks/process_audio.py`

**å½“å‰ç”Ÿæˆæ‘˜è¦çš„ä»£ç ä½ç½®ï¼š** çº¦750-842è¡Œ

**éœ€è¦ä¿®æ”¹ä¸ºä¸¤é˜¶æ®µç”Ÿæˆï¼š**

```python
# ===== é˜¶æ®µ1ï¼šè´¨é‡è¯„ä¼°å’Œé¢„å¤„ç† =====
from app.utils.transcript_processor import TranscriptProcessor

# è¯„ä¼°è½¬å†™è´¨é‡
quality = TranscriptProcessor.assess_quality(segments)

logger.info(
    f"Task {task_id}: Transcript quality - "
    f"{quality.quality_score} (confidence: {quality.avg_confidence:.2f})"
)

# é¢„å¤„ç†è½¬å†™æ–‡æœ¬
preprocessed_text = TranscriptProcessor.preprocess(
    segments,
    filter_filler_words=True,
    merge_same_speaker=True,
    merge_threshold_seconds=2.0
)

# ç”Ÿæˆè´¨é‡æç¤º
quality_notice = TranscriptProcessor.get_quality_notice(quality)

# ===== é˜¶æ®µ2ï¼šæ ¹æ®è´¨é‡é€‰æ‹©LLMæœåŠ¡ =====
if quality.quality_score == "low":
    # ä½è´¨é‡ï¼šä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹
    llm_service = await SmartFactory.get_service(
        "llm",
        provider="openrouter",
        model_id="anthropic/claude-3.5-sonnet"
    )
    logger.warning(
        f"Task {task_id}: Low quality transcript detected, using premium model"
    )
else:
    # æ­£å¸¸è´¨é‡ï¼šä½¿ç”¨å¸¸è§„æœåŠ¡
    llm_service = await SmartFactory.get_service("llm", provider=provider, model_id=model_id)

# ===== é˜¶æ®µ3ï¼šç« èŠ‚åˆ’åˆ†ï¼ˆæ–°å¢ï¼‰ =====

# æ³¨æ„ï¼šåªå¯¹é•¿å†…å®¹ï¼ˆ>2000å­—ç¬¦ï¼‰è¿›è¡Œç« èŠ‚åˆ’åˆ†
if len(preprocessed_text) > 2000:
    try:
        # è·å–ç« èŠ‚åˆ’åˆ†æç¤ºè¯
        segmentation_prompt = get_prompt_manager().get_prompt(
            category="segmentation",
            prompt_type="segment",
            locale="zh-CN",
            variables={
                "transcript": preprocessed_text,
                "quality_notice": quality_notice
            },
            content_style=content_style
        )

        # æ„å»ºå®Œæ•´promptï¼ˆsystem + userï¼‰
        full_prompt = f"{segmentation_prompt['system']}\n\n{segmentation_prompt['user_prompt']}"

        # è°ƒç”¨LLMè¿›è¡Œç« èŠ‚åˆ’åˆ†
        segmentation_result = await llm_service.generate(
            prompt=full_prompt,
            temperature=0.3,
            max_tokens=1500
        )

        # è§£æJSONç»“æœ
        import json
        chapters_data = json.loads(segmentation_result)

        # å­˜å‚¨ç« èŠ‚ä¿¡æ¯åˆ°æ•°æ®åº“
        chapters_summary = Summary(
            task_id=task.id,
            summary_type="chapters",
            version=1,
            is_active=True,
            content=segmentation_result,  # å­˜å‚¨å®Œæ•´JSON
            model_used=llm_service.model_name,
            prompt_version="v1.2.0",
            token_count=len(segmentation_result)
        )
        session.add(chapters_summary)
        await session.commit()

        logger.info(
            f"Task {task_id}: Chapter segmentation completed - "
            f"{chapters_data['total_chapters']} chapters identified"
        )

    except Exception as e:
        logger.warning(
            f"Task {task_id}: Chapter segmentation failed: {e}",
            exc_info=True
        )
        # ç« èŠ‚åˆ’åˆ†å¤±è´¥ä¸å½±å“åç»­æ‘˜è¦ç”Ÿæˆ
        chapters_data = None
else:
    # çŸ­å†…å®¹è·³è¿‡ç« èŠ‚åˆ’åˆ†
    chapters_data = None
    logger.info(
        f"Task {task_id}: Content too short ({len(preprocessed_text)} chars), "
        "skipping chapter segmentation"
    )

# ===== é˜¶æ®µ4ï¼šç”Ÿæˆå„ç±»æ‘˜è¦ï¼ˆä¿®æ”¹ç°æœ‰é€»è¾‘ï¼‰ =====

summaries = []
llm_usages = []

for summary_type in ("overview", "key_points", "action_items"):
    logger.info(
        f"Task {task_id}: Generating {summary_type} summary (style: {content_style})"
    )

    # è·å–æç¤ºè¯ï¼ˆç°åœ¨åŒ…å«quality_noticeå’Œchaptersæ”¯æŒï¼‰
    prompt_config = get_prompt_manager().get_prompt(
        category="summary",
        prompt_type=summary_type,
        locale="zh-CN",
        variables={
            "transcript": preprocessed_text,  # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡æœ¬
            "quality_notice": quality_notice   # è´¨é‡æç¤º
        },
        content_style=content_style
    )

    # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
    try:
        # æ„å»ºå®Œæ•´prompt
        system_role = prompt_config["system"]
        user_prompt = prompt_config["user_prompt"]

        # æ ¹æ®LLMæœåŠ¡çš„å®ç°æ–¹å¼è°ƒç”¨
        if hasattr(llm_service, 'summarize'):
            # å¦‚æœLLMæœåŠ¡æœ‰summarizeæ–¹æ³•ï¼Œä½¿ç”¨ç°æœ‰é€»è¾‘
            content = await llm_service.summarize(preprocessed_text, summary_type, content_style)
        else:
            # å¦åˆ™ä½¿ç”¨æ–°çš„generateæ–¹æ³•
            full_prompt = f"{system_role}\n\n{user_prompt}"
            content = await llm_service.generate(
                prompt=full_prompt,
                temperature=prompt_config["model_params"].get("temperature"),
                max_tokens=prompt_config["model_params"].get("max_tokens")
            )

        # è®°å½•æˆåŠŸ
        if llm_provider:
            input_tokens = len(preprocessed_text)
            output_tokens = len(content)
            estimated_cost = 0.0
            if hasattr(llm_service, "estimate_cost"):
                estimated_cost = llm_service.estimate_cost(input_tokens, output_tokens)

            llm_usages.append(
                LLMUsage(
                    user_id=str(task.user_id),
                    task_id=str(task.id),
                    provider=llm_provider,
                    model_id=llm_service.model_name,
                    call_type="summarize",
                    summary_type=summary_type,
                    status="success",
                )
            )

        logger.info(
            f"Task {task_id}: Generated {summary_type} summary ({len(content)} characters)"
        )

        summaries.append(
            Summary(
                task_id=task.id,
                summary_type=summary_type,
                version=1,
                is_active=True,
                content=content,
                model_used=llm_service.model_name,
                prompt_version="v1.2.0",
                token_count=len(content),
            )
        )

    except Exception as exc:
        logger.error(
            f"Task {task_id}: Failed to generate {summary_type} summary: {exc}",
            exc_info=True
        )
        if llm_provider:
            llm_usages.append(
                LLMUsage(
                    user_id=str(task.user_id),
                    task_id=str(task.id),
                    provider=llm_provider,
                    model_id=llm_service.model_name,
                    call_type="summarize",
                    summary_type=summary_type,
                    status="failed",
                )
            )
        # ç»§ç»­å¤„ç†å…¶ä»–summary_type

# ä¿å­˜æ‰€æœ‰æ‘˜è¦
session.add_all(summaries)
if llm_usages:
    session.add_all(llm_usages)
await session.commit()

logger.info(
    f"Task {task_id}: All summaries saved to database",
    extra={"task_id": task_id, "summary_count": len(summaries)}
)
```

---

### ç¬¬å››æ­¥ï¼šæ·»åŠ APIç«¯ç‚¹è·å–ç« èŠ‚ä¿¡æ¯

**æ–‡ä»¶ï¼š** `app/api/v1/summaries.py`

**æ–°å¢ç«¯ç‚¹ï¼š**

```python
@router.get("/{task_id}/chapters", response_model=dict)
async def get_task_chapters(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    """è·å–ä»»åŠ¡çš„ç« èŠ‚åˆ’åˆ†ä¿¡æ¯

    Returns:
        {
            "code": 0,
            "data": {
                "total_chapters": 3,
                "chapters": [...]
            }
        }
    """
    # æŸ¥è¯¢chaptersç±»å‹çš„summary
    result = await db.execute(
        select(Summary).where(
            Summary.task_id == task_id,
            Summary.summary_type == "chapters",
            Summary.is_active == True,
            Summary.deleted_at.is_(None)
        )
    )
    chapters_summary = result.scalar_one_or_none()

    if not chapters_summary:
        return success(data=None, message="æš‚æ— ç« èŠ‚åˆ’åˆ†ä¿¡æ¯")

    # è§£æJSON
    import json
    chapters_data = json.loads(chapters_summary.content)

    return success(data=chapters_data)
```

---

## æµ‹è¯•è®¡åˆ’ ğŸ§ª

### å•å…ƒæµ‹è¯•

**æµ‹è¯•æ–‡ä»¶ï¼š** `tests/test_transcript_processor.py`

```python
import pytest
from app.services.asr.base import TranscriptSegment
from app.utils.transcript_processor import TranscriptProcessor, TranscriptQuality

def test_assess_quality_high():
    segments = [
        TranscriptSegment("speaker_0", 0.0, 3.0, "å¤§å®¶å¥½", 0.95, None),
        TranscriptSegment("speaker_0", 3.0, 6.0, "ä»Šå¤©æˆ‘ä»¬è®¨è®º", 0.92, None),
    ]
    quality = TranscriptProcessor.assess_quality(segments)
    assert quality.quality_score == "high"
    assert quality.avg_confidence >= 0.8

def test_assess_quality_low():
    segments = [
        TranscriptSegment("speaker_0", 0.0, 3.0, "å¤§å®¶å¥½", 0.45, None),
        TranscriptSegment("speaker_0", 3.0, 6.0, "ä»Šå¤©æˆ‘ä»¬è®¨è®º", 0.52, None),
    ]
    quality = TranscriptProcessor.assess_quality(segments)
    assert quality.quality_score == "low"
    assert quality.avg_confidence < 0.6

def test_preprocess_filter_filler_words():
    segments = [
        TranscriptSegment("speaker_0", 0.0, 1.0, "å—¯", 0.65, None),
        TranscriptSegment("speaker_0", 1.0, 4.0, "å¤§å®¶å¥½", 0.92, None),
        TranscriptSegment("speaker_0", 4.0, 5.0, "å•Š", 0.60, None),
    ]
    preprocessed = TranscriptProcessor.preprocess(segments)
    assert "å—¯" not in preprocessed
    assert "å•Š" not in preprocessed
    assert "å¤§å®¶å¥½" in preprocessed

def test_preprocess_merge_segments():
    segments = [
        TranscriptSegment("speaker_0", 0.0, 3.0, "å¤§å®¶å¥½", 0.92, None),
        TranscriptSegment("speaker_0", 3.5, 6.0, "ä»Šå¤©æˆ‘ä»¬è®¨è®º", 0.90, None),
        TranscriptSegment("speaker_1", 6.5, 9.0, "å¥½çš„", 0.88, None),
    ]
    preprocessed = TranscriptProcessor.preprocess(segments, merge_threshold_seconds=1.0)
    # speaker_0çš„ä¸¤ä¸ªsegmentåº”è¯¥è¢«åˆå¹¶
    assert "[speaker_0] å¤§å®¶å¥½ ä»Šå¤©æˆ‘ä»¬è®¨è®º" in preprocessed
    assert "[speaker_1] å¥½çš„" in preprocessed
```

---

### é›†æˆæµ‹è¯•

1. **æµ‹è¯•çŸ­å†…å®¹ï¼ˆ<2000å­—ç¬¦ï¼‰**
   - åº”è¯¥è·³è¿‡ç« èŠ‚åˆ’åˆ†
   - ç”Ÿæˆ3ç§æ‘˜è¦

2. **æµ‹è¯•ä¸­ç­‰å†…å®¹ï¼ˆ2000-5000å­—ç¬¦ï¼‰**
   - åº”è¯¥ç”Ÿæˆ2-3ä¸ªç« èŠ‚
   - ç”Ÿæˆ4ç§å†…å®¹ï¼ˆchapters + 3ç§æ‘˜è¦ï¼‰

3. **æµ‹è¯•é•¿å†…å®¹ï¼ˆ>5000å­—ç¬¦ï¼‰**
   - åº”è¯¥ç”Ÿæˆ3-5ä¸ªç« èŠ‚
   - éªŒè¯ç« èŠ‚åˆ’åˆ†çš„åˆç†æ€§

4. **æµ‹è¯•ä½è´¨é‡è½¬å†™**
   - æ„é€ ä½ç½®ä¿¡åº¦çš„segments
   - éªŒè¯æ˜¯å¦ä½¿ç”¨äº†premium model
   - éªŒè¯quality_noticeæ˜¯å¦æ­£ç¡®ç”Ÿæˆ

5. **æµ‹è¯•ä¸‰ç§é£æ ¼**
   - meeting: éªŒè¯è¾“å‡ºåŒ…å«å†³ç­–ã€å¾…åŠã€é—ç•™é—®é¢˜
   - lecture: éªŒè¯è¾“å‡ºåŒ…å«æ¦‚å¿µã€çŸ¥è¯†ç‚¹ã€å­¦ä¹ é‡ç‚¹
   - podcast: éªŒè¯è¾“å‡ºåŒ…å«è§‚ç‚¹ã€é‡‘å¥ã€å¯ç¤º

---

## å›æ»šæ–¹æ¡ˆ ğŸ”„

å¦‚æœæ–°ç‰ˆæœ¬å‡ºç°é—®é¢˜ï¼Œå¯ä»¥å¿«é€Ÿå›æ»šï¼š

```bash
cd /Users/sean/.claude-worktrees/ai-audio-assistant-web/amazing-hawking

# å›æ»šæç¤ºè¯
cd app/prompts/templates/summary
mv zh-CN.json zh-CN-v1.2-failed.json
mv zh-CN-v1.1-backup.json zh-CN.json

# å›æ»šconfig.json
git checkout app/prompts/templates/summary/config.json

# åˆ é™¤æ–°å¢çš„segmentationæç¤ºè¯
rm -rf app/prompts/templates/segmentation

# åˆ é™¤transcript_processor
rm app/utils/transcript_processor.py

# å›æ»šworkerä»£ç 
git checkout worker/tasks/process_audio.py
```

---

## é¢„æœŸæ•ˆæœ ğŸ“Š

### æˆæœ¬å˜åŒ–
- **æ­£å¸¸è´¨é‡ï¼ˆ80%ï¼‰ï¼š** å‡å°‘10-15%ï¼ˆè¿‡æ»¤è¯­æ°”è¯ï¼‰
- **ä½è´¨é‡ï¼ˆ20%ï¼‰ï¼š** å¢åŠ çº¦170%ï¼ˆä½¿ç”¨premium modelï¼‰
- **ç»¼åˆæˆæœ¬ï¼š** å¢åŠ çº¦26%

### è´¨é‡æå‡
- âœ… è¾“å‡ºç»“æ„åŒ–ç¨‹åº¦æ˜¾è‘—æå‡
- âœ… ä¸‰ç§é£æ ¼å·®å¼‚åŒ–æ˜æ˜¾
- âœ… é•¿å†…å®¹é€šè¿‡ç« èŠ‚åŒ–æ›´æ˜“é˜…è¯»
- âœ… ä½è´¨é‡è½¬å†™çš„å¤„ç†èƒ½åŠ›å¢å¼º
- âœ… è¡¨æ ¼ã€emojiç­‰å…ƒç´ æå‡å¯è¯»æ€§

### Tokenæ¶ˆè€—
- Overview: 500 â†’ 1500 tokens (3x)
- Key Points: 800 â†’ 1200 tokens (1.5x)
- Action Items: 600 â†’ 1000 tokens (1.67x)
- **æ€»è¾“å‡ºtokenå¢åŠ çº¦2x**

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨ âœˆï¸

1. **ç«‹å³å®æ–½**ï¼šå®Œæˆå‰©ä½™çš„4ä¸ªæ­¥éª¤
2. **æœ¬åœ°æµ‹è¯•**ï¼šä½¿ç”¨æµ‹è¯•æ•°æ®éªŒè¯åŠŸèƒ½
3. **è°ƒä¼˜æç¤ºè¯**ï¼šæ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´
4. **å‘å¸ƒåˆ°æµ‹è¯•ç¯å¢ƒ**
5. **æ”¶é›†ç”¨æˆ·åé¦ˆ**
6. **è¿­ä»£ä¼˜åŒ–**

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** V1.0
**åˆ›å»ºæ—¥æœŸï¼š** 2025-01-15
**ä½œè€…ï¼š** Claude Code
